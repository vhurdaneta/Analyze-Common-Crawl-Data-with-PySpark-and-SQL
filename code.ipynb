{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d31362fa",
   "metadata": {},
   "source": [
    "# PySpark and Big Data Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d3f30bf",
   "metadata": {},
   "source": [
    "Analyzing Common Crawl Data with RDDs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d0c96e6",
   "metadata": {},
   "source": [
    "Initialize a new Spark Context and read in the domain graph as an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8d3ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/04 12:20:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Configurar el nivel de registro a un valor diferente (por ejemplo, \"INFO\")\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ba7d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:20:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "23/09/04 12:20:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "23/09/04 12:20:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.123:52281 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:20:59 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:20:59 INFO FileInputFormat: Total input files to process : 1\n",
      "23/09/04 12:20:59 INFO SparkContext: Starting job: runJob at PythonRDD.scala:179\n",
      "23/09/04 12:20:59 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:179) with 1 output partitions\n",
      "23/09/04 12:20:59 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:179)\n",
      "23/09/04 12:20:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:20:59 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:20:59 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "23/09/04 12:20:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.9 KiB, free 434.1 MiB)\n",
      "23/09/04 12:20:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.1 MiB)\n",
      "23/09/04 12:20:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.123:52281 (size: 4.8 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:20:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:20:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:20:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:20:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 7536 bytes) \n",
      "23/09/04 12:20:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/09/04 12:20:59 INFO HadoopRDD: Input split: file:/Users/victorurdaneta/Desktop/Documentos Personales/Personal Growth/CodeAcademy/Analyze Common Crawl Data with PySpark/data/cc-main-limited-domains.csv:0+636905\n",
      "23/09/04 12:21:00 INFO PythonRunner: Times: total = 931, boot = 780, init = 151, finish = 0\n",
      "23/09/04 12:21:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1618 bytes result sent to driver\n",
      "23/09/04 12:21:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1114 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:00 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52282\n",
      "23/09/04 12:21:00 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:179) finished in 1.263 s\n",
      "23/09/04 12:21:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/09/04 12:21:00 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:179, took 1.310913 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['367855\\t172-in-addr\\tarpa\\t1',\n",
       " '367856\\taddr\\tarpa\\t1',\n",
       " '367857\\tamphic\\tarpa\\t1',\n",
       " '367858\\tbeta\\tarpa\\t1',\n",
       " '367859\\tcallic\\tarpa\\t1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Domains CSV File into an RDD\n",
    "file_path = 'data/cc-main-limited-domains.csv'\n",
    "common_crawl_domain_counts = sc.textFile(file_path)\n",
    "\n",
    "# Display first few domains from the RDD\n",
    "common_crawl_domain_counts.take(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f18a601",
   "metadata": {},
   "source": [
    "Applying `fmt_domain_graph_entry` over `common_crawl_domain_counts` and saveing the result as a new RDD named `formatted_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7950d8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:179\n",
      "23/09/04 12:21:00 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:179) with 1 output partitions\n",
      "23/09/04 12:21:00 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:179)\n",
      "23/09/04 12:21:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:00 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:00 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "23/09/04 12:21:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.8 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.123:52281 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:00 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[3] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 7536 bytes) \n",
      "23/09/04 12:21:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "23/09/04 12:21:00 INFO HadoopRDD: Input split: file:/Users/victorurdaneta/Desktop/Documentos Personales/Personal Growth/CodeAcademy/Analyze Common Crawl Data with PySpark/data/cc-main-limited-domains.csv:0+636905\n",
      "23/09/04 12:21:01 INFO PythonRunner: Times: total = 143, boot = 6, init = 136, finish = 1\n",
      "23/09/04 12:21:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1735 bytes result sent to driver\n",
      "23/09/04 12:21:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 165 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:01 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:179) finished in 0.178 s\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:179, took 0.185273 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(367855, '172-in-addr', 'arpa', 1),\n",
       " (367856, 'addr', 'arpa', 1),\n",
       " (367857, 'amphic', 'arpa', 1),\n",
       " (367858, 'beta', 'arpa', 1),\n",
       " (367859, 'callic', 'arpa', 1),\n",
       " (367860, 'ch', 'arpa', 1),\n",
       " (367861, 'd', 'arpa', 1),\n",
       " (367862, 'home', 'arpa', 7),\n",
       " (367863, 'iana', 'arpa', 1),\n",
       " (367907, 'local', 'arpa', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fmt_domain_graph_entry(entry):\n",
    "    \"\"\"\n",
    "    Formats a Common Crawl domain graph entry. Extracts the site_id, \n",
    "    top-level domain (tld), domain name, and subdomain count as seperate items.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')        \n",
    "    return int(site_id), domain, tld, int(num_subdomains)\n",
    "\n",
    "# Apply `fmt_domain_graph_entry` to the raw data RDD\n",
    "formatted_host_counts = common_crawl_domain_counts.map(fmt_domain_graph_entry)\n",
    "\n",
    "# Display the first few entries of the new RDD\n",
    "formatted_host_counts.take(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e9a9aae",
   "metadata": {},
   "source": [
    "Applying `extract_subdomain_counts` over `common_crawl_domain_counts` and saving the result as a new RDD named `host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb75dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:01 INFO SparkContext: Starting job: runJob at PythonRDD.scala:179\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:179) with 1 output partitions\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at PythonRDD.scala:179)\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[4] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "23/09/04 12:21:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.7 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.123:52281 (size: 5.4 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (PythonRDD[4] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:01 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:01 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 7536 bytes) \n",
      "23/09/04 12:21:01 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "23/09/04 12:21:01 INFO HadoopRDD: Input split: file:/Users/victorurdaneta/Desktop/Documentos Personales/Personal Growth/CodeAcademy/Analyze Common Crawl Data with PySpark/data/cc-main-limited-domains.csv:0+636905\n",
      "23/09/04 12:21:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.123:52281 in memory (size: 5.5 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.123:52281 in memory (size: 4.8 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:01 INFO PythonRunner: Times: total = 143, boot = 6, init = 137, finish = 0\n",
      "23/09/04 12:21:01 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1505 bytes result sent to driver\n",
      "23/09/04 12:21:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 174 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:01 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:01 INFO DAGScheduler: ResultStage 2 (runJob at PythonRDD.scala:179) finished in 0.194 s\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:179, took 0.200181 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 7, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subdomain_counts(entry):\n",
    "    \"\"\"\n",
    "    Extract the subdomain count from a Common Crawl domain graph entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')\n",
    "    \n",
    "    # return ONLY the num_subdomains\n",
    "    return int(num_subdomains)\n",
    "\n",
    "\n",
    "# Apply `extract_subdomain_counts` to the raw data RDD\n",
    "host_counts = common_crawl_domain_counts.map(extract_subdomain_counts)\n",
    "\n",
    "# Display the first few entries\n",
    "host_counts.take(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e44483f3",
   "metadata": {},
   "source": [
    "Using `host_counts`, calculate the total number of subdomains across all domains in the dataset, save the result to a variable named `total_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa284001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:01 INFO SparkContext: Starting job: reduce at /var/folders/j0/7nr6h2fd7k18y1yvw_s7dm540000gn/T/ipykernel_7216/3125385089.py:2\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Got job 3 (reduce at /var/folders/j0/7nr6h2fd7k18y1yvw_s7dm540000gn/T/ipykernel_7216/3125385089.py:2) with 2 output partitions\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Final stage: ResultStage 3 (reduce at /var/folders/j0/7nr6h2fd7k18y1yvw_s7dm540000gn/T/ipykernel_7216/3125385089.py:2)\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[5] at reduce at /var/folders/j0/7nr6h2fd7k18y1yvw_s7dm540000gn/T/ipykernel_7216/3125385089.py:2), which has no missing parents\n",
      "23/09/04 12:21:01 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.3 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:01 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.123:52281 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:01 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.123:52281 in memory (size: 5.4 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:01 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[5] at reduce at /var/folders/j0/7nr6h2fd7k18y1yvw_s7dm540000gn/T/ipykernel_7216/3125385089.py:2) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/09/04 12:21:01 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
      "23/09/04 12:21:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 7536 bytes) \n",
      "23/09/04 12:21:01 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (192.168.1.123, executor driver, partition 1, PROCESS_LOCAL, 7536 bytes) \n",
      "23/09/04 12:21:01 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "23/09/04 12:21:01 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)\n",
      "23/09/04 12:21:01 INFO HadoopRDD: Input split: file:/Users/victorurdaneta/Desktop/Documentos Personales/Personal Growth/CodeAcademy/Analyze Common Crawl Data with PySpark/data/cc-main-limited-domains.csv:0+636905\n",
      "23/09/04 12:21:01 INFO HadoopRDD: Input split: file:/Users/victorurdaneta/Desktop/Documentos Personales/Personal Growth/CodeAcademy/Analyze Common Crawl Data with PySpark/data/cc-main-limited-domains.csv:636905+636906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:01 INFO PythonRunner: Times: total = 205, boot = 9, init = 146, finish = 50\n",
      "23/09/04 12:21:01 INFO PythonRunner: Times: total = 211, boot = 8, init = 148, finish = 55\n",
      "23/09/04 12:21:01 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 1412 bytes result sent to driver\n",
      "23/09/04 12:21:01 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1498 bytes result sent to driver\n",
      "23/09/04 12:21:01 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 237 ms on 192.168.1.123 (executor driver) (1/2)\n",
      "23/09/04 12:21:01 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 238 ms on 192.168.1.123 (executor driver) (2/2)\n",
      "23/09/04 12:21:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:01 INFO DAGScheduler: ResultStage 3 (reduce at /var/folders/j0/7nr6h2fd7k18y1yvw_s7dm540000gn/T/ipykernel_7216/3125385089.py:2) finished in 0.256 s\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "23/09/04 12:21:01 INFO DAGScheduler: Job 3 finished: reduce at /var/folders/j0/7nr6h2fd7k18y1yvw_s7dm540000gn/T/ipykernel_7216/3125385089.py:2, took 0.261378 s\n"
     ]
    }
   ],
   "source": [
    "# Reduce the RDD to a single value, the sum of subdomains, with a lambda function as the reduce function\n",
    "total_host_counts = host_counts.reduce(lambda a,b: a+b)\n",
    "\n",
    "# Display result count\n",
    "print(total_host_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11f5579e",
   "metadata": {},
   "source": [
    "Stop the current `SparkSession` and `sparkContext` before moving on to analyze the data with SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562745d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:01 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "23/09/04 12:21:01 INFO SparkUI: Stopped Spark web UI at http://192.168.1.123:4040\n",
      "23/09/04 12:21:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/09/04 12:21:01 INFO MemoryStore: MemoryStore cleared\n",
      "23/09/04 12:21:01 INFO BlockManager: BlockManager stopped\n",
      "23/09/04 12:21:01 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/09/04 12:21:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/09/04 12:21:01 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# Stop the sparkContext and the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f129687d",
   "metadata": {},
   "source": [
    "## Exploring Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c22e2b6",
   "metadata": {},
   "source": [
    "Create a new `SparkSession` and assign it to a variable named `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99565365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:02 INFO SparkContext: Running Spark version 3.4.1\n",
      "23/09/04 12:21:02 INFO ResourceUtils: ==============================================================\n",
      "23/09/04 12:21:02 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/09/04 12:21:02 INFO ResourceUtils: ==============================================================\n",
      "23/09/04 12:21:02 INFO SparkContext: Submitted application: pyspark-shell\n",
      "23/09/04 12:21:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/09/04 12:21:02 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/09/04 12:21:02 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/09/04 12:21:02 INFO SecurityManager: Changing view acls to: victorurdaneta\n",
      "23/09/04 12:21:02 INFO SecurityManager: Changing modify acls to: victorurdaneta\n",
      "23/09/04 12:21:02 INFO SecurityManager: Changing view acls groups to: \n",
      "23/09/04 12:21:02 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/09/04 12:21:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: victorurdaneta; groups with view permissions: EMPTY; users with modify permissions: victorurdaneta; groups with modify permissions: EMPTY\n",
      "23/09/04 12:21:02 INFO Utils: Successfully started service 'sparkDriver' on port 52299.\n",
      "23/09/04 12:21:02 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/09/04 12:21:02 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/09/04 12:21:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/09/04 12:21:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/09/04 12:21:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/09/04 12:21:02 INFO DiskBlockManager: Created local directory at /private/var/folders/j0/7nr6h2fd7k18y1yvw_s7dm540000gn/T/blockmgr-9fc0df47-b185-4f6c-92a9-6cb8770f578f\n",
      "23/09/04 12:21:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/09/04 12:21:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/09/04 12:21:02 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "23/09/04 12:21:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/09/04 12:21:02 INFO Executor: Starting executor ID driver on host 192.168.1.123\n",
      "23/09/04 12:21:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/09/04 12:21:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52300.\n",
      "23/09/04 12:21:02 INFO NettyBlockTransferService: Server created on 192.168.1.123:52300\n",
      "23/09/04 12:21:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/09/04 12:21:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.123, 52300, None)\n",
      "23/09/04 12:21:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.123:52300 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.1.123, 52300, None)\n",
      "23/09/04 12:21:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.123, 52300, None)\n",
      "23/09/04 12:21:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.123, 52300, None)\n",
      "23/09/04 12:21:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/09/04 12:21:02 INFO SharedState: Warehouse path is 'file:/Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/spark-warehouse'.\n",
      "23/09/04 12:21:03 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.\n",
      "23/09/04 12:21:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "23/09/04 12:21:06 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:06 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "23/09/04 12:21:06 INFO CodeGenerator: Code generated in 226.122933 ms\n",
      "23/09/04 12:21:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.8 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.123:52300 (size: 34.6 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:06 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:06 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:06 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:06 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:06 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.123:52300 (size: 6.0 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8052 bytes) \n",
      "23/09/04 12:21:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/09/04 12:21:06 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/data/cc-main-limited-domains.csv, range: 0-1273811, partition values: [empty row]\n",
      "23/09/04 12:21:06 INFO CodeGenerator: Code generated in 20.007305 ms\n",
      "23/09/04 12:21:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1687 bytes result sent to driver\n",
      "23/09/04 12:21:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 119 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:07 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.154 s\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.160687 s\n",
      "23/09/04 12:21:07 INFO CodeGenerator: Code generated in 14.026958 ms\n",
      "23/09/04 12:21:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.123:52300 in memory (size: 6.0 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:07 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:07 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/04 12:21:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.8 KiB, free 434.0 MiB)\n",
      "23/09/04 12:21:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.123:52300 (size: 34.6 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:07 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.2 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.123:52300 in memory (size: 34.6 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.123:52300 (size: 12.3 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8052 bytes) \n",
      "23/09/04 12:21:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "23/09/04 12:21:07 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/data/cc-main-limited-domains.csv, range: 0-1273811, partition values: [empty row]\n",
      "23/09/04 12:21:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1664 bytes result sent to driver\n",
      "23/09/04 12:21:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 419 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:07 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.464 s\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0.469730 s\n",
      "23/09/04 12:21:07 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:07 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/04 12:21:07 INFO CodeGenerator: Code generated in 20.983168 ms\n",
      "23/09/04 12:21:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.6 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:07 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.123:52300 in memory (size: 12.3 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.123:52300 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:07 INFO SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+---+\n",
      "|_c0   |_c1        |_c2 |_c3|\n",
      "+------+-----------+----+---+\n",
      "|367855|172-in-addr|arpa|1  |\n",
      "|367856|addr       |arpa|1  |\n",
      "|367857|amphic     |arpa|1  |\n",
      "|367858|beta       |arpa|1  |\n",
      "|367859|callic     |arpa|1  |\n",
      "+------+-----------+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:07 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.0 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.123:52300 (size: 7.4 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.123:52300 in memory (size: 34.6 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8052 bytes) \n",
      "23/09/04 12:21:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "23/09/04 12:21:07 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/data/cc-main-limited-domains.csv, range: 0-1273811, partition values: [empty row]\n",
      "23/09/04 12:21:07 INFO CodeGenerator: Code generated in 18.206509 ms\n",
      "23/09/04 12:21:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1719 bytes result sent to driver\n",
      "23/09/04 12:21:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 70 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:08 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.086 s\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.090325 s\n",
      "23/09/04 12:21:08 INFO CodeGenerator: Code generated in 22.05194 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the target file into a DataFrame\n",
    "common_crawl = spark.read\\\n",
    ".option('delimiter', '\\t')\\\n",
    ".option('inferSchema', True)\\\n",
    ".csv(file_path)\n",
    "\n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "common_crawl.show(5, truncate = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23082fd2",
   "metadata": {},
   "source": [
    "Rename the DataFrame's columns to the following: \n",
    "\n",
    "- site_id\n",
    "- domain\n",
    "- top_level_domain\n",
    "- num_subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7b4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|domain     |top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "|367855 |172-in-addr|arpa            |1             |\n",
      "|367856 |addr       |arpa            |1             |\n",
      "|367857 |amphic     |arpa            |1             |\n",
      "|367858 |beta       |arpa            |1             |\n",
      "|367859 |callic     |arpa            |1             |\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:08 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:08 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/04 12:21:08 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 199.6 KiB, free 434.0 MiB)\n",
      "23/09/04 12:21:08 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.123:52300 in memory (size: 7.4 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.123:52300 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:08 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.123:52300 (size: 34.5 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:08 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:08 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:08 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 15.0 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:08 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:08 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.123:52300 (size: 7.4 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:08 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:08 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:08 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8052 bytes) \n",
      "23/09/04 12:21:08 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "23/09/04 12:21:08 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/data/cc-main-limited-domains.csv, range: 0-1273811, partition values: [empty row]\n",
      "23/09/04 12:21:08 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1676 bytes result sent to driver\n",
      "23/09/04 12:21:08 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 19 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:08 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:08 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.034 s\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.038213 s\n"
     ]
    }
   ],
   "source": [
    "# Rename the DataFrame's columns with `withColumnRenamed()`\n",
    "common_crawl =common_crawl\\\n",
    ".withColumnRenamed(\"_c0\", \"site_id\")\\\n",
    ".withColumnRenamed(\"_c1\", \"domain\")\\\n",
    ".withColumnRenamed(\"_c2\", \"top_level_domain\")\\\n",
    ".withColumnRenamed(\"_c3\", \"num_subdomains\")\\\n",
    "\n",
    "# Display the first few rows of the DataFrame and the new schema\n",
    "common_crawl.show(5, truncate = False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff524e08",
   "metadata": {},
   "source": [
    "## Reading and Writing Datasets to Disk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a00bc518",
   "metadata": {},
   "source": [
    "Saveing, reading and displaying the `common_crawl` DataFrame as parquet files in a directory called `./results/common_crawl/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33be3162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:08 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:08 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/04 12:21:08 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/04 12:21:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/09/04 12:21:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/09/04 12:21:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/04 12:21:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/09/04 12:21:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/09/04 12:21:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/04 12:21:08 INFO CodeGenerator: Code generated in 14.471897 ms\n",
      "23/09/04 12:21:08 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 199.6 KiB, free 434.0 MiB)\n",
      "23/09/04 12:21:08 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.1.123:52300 in memory (size: 7.4 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:08 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.123:52300 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:08 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.123:52300 (size: 34.5 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:08 INFO SparkContext: Created broadcast 8 from parquet at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:08 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:08 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 216.2 KiB, free 434.0 MiB)\n",
      "23/09/04 12:21:08 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 78.3 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:08 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.123:52300 (size: 78.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:08 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:08 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8052 bytes) \n",
      "23/09/04 12:21:08 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "23/09/04 12:21:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/09/04 12:21:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/09/04 12:21:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/04 12:21:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/09/04 12:21:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/09/04 12:21:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/04 12:21:08 INFO CodecConfig: Compression: SNAPPY\n",
      "23/09/04 12:21:08 INFO CodecConfig: Compression: SNAPPY\n",
      "23/09/04 12:21:08 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "23/09/04 12:21:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"site_id\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"domain\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"top_level_domain\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"num_subdomains\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 site_id;\n",
      "  optional binary domain (STRING);\n",
      "  optional binary top_level_domain (STRING);\n",
      "  optional int32 num_subdomains;\n",
      "}\n",
      "\n",
      "       \n",
      "23/09/04 12:21:08 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "23/09/04 12:21:09 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/data/cc-main-limited-domains.csv, range: 0-1273811, partition values: [empty row]\n",
      "23/09/04 12:21:10 INFO FileOutputCommitter: Saved output of task 'attempt_202309041221086302556512346262130_0004_m_000000_4' to file:/Users/victorurdaneta/Desktop/Documentos Personales/Personal Growth/CodeAcademy/Analyze Common Crawl Data with PySpark/results/common_crawl/_temporary/0/task_202309041221086302556512346262130_0004_m_000000\n",
      "23/09/04 12:21:10 INFO SparkHadoopMapRedUtil: attempt_202309041221086302556512346262130_0004_m_000000_4: Committed. Elapsed time: 1 ms.\n",
      "23/09/04 12:21:10 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2613 bytes result sent to driver\n",
      "23/09/04 12:21:10 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1398 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:10 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:10 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.469 s\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.474038 s\n",
      "23/09/04 12:21:10 INFO FileFormatWriter: Start to commit write Job c8844173-f849-4aab-aa3e-980509a89280.\n",
      "23/09/04 12:21:10 INFO FileFormatWriter: Write Job c8844173-f849-4aab-aa3e-980509a89280 committed. Elapsed time: 23 ms.\n",
      "23/09/04 12:21:10 INFO FileFormatWriter: Finished processing stats for write job c8844173-f849-4aab-aa3e-980509a89280.\n",
      "23/09/04 12:21:10 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "23/09/04 12:21:10 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:10 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.4 KiB, free 433.8 MiB)\n",
      "23/09/04 12:21:10 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 433.7 MiB)\n",
      "23/09/04 12:21:10 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.1.123:52300 (size: 37.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:10 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:10 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:10 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 7670 bytes) \n",
      "23/09/04 12:21:10 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "23/09/04 12:21:10 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1914 bytes result sent to driver\n",
      "23/09/04 12:21:10 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 48 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:10 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:10 INFO DAGScheduler: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.075 s\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.079042 s\n",
      "23/09/04 12:21:10 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:10 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/04 12:21:10 INFO CodeGenerator: Code generated in 25.536372 ms\n",
      "23/09/04 12:21:10 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 201.4 KiB, free 433.5 MiB)\n",
      "23/09/04 12:21:10 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.1.123:52300 in memory (size: 37.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:10 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 433.7 MiB)\n",
      "23/09/04 12:21:10 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.1.123:52300 (size: 35.2 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:10 INFO SparkContext: Created broadcast 11 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:10 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:10 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 14.9 KiB, free 433.6 MiB)\n",
      "23/09/04 12:21:10 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 433.6 MiB)\n",
      "23/09/04 12:21:10 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.1.123:52300 (size: 6.3 KiB, free: 434.2 MiB)\n",
      "23/09/04 12:21:10 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:10 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:10 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes) \n",
      "23/09/04 12:21:10 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
      "23/09/04 12:21:10 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/results/common_crawl/part-00000-d507c219-514d-47a0-9cdd-d3c7b288e82d-c000.snappy.parquet, range: 0-639764, partition values: [empty row]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|domain     |top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "|367855 |172-in-addr|arpa            |1             |\n",
      "|367856 |addr       |arpa            |1             |\n",
      "|367857 |amphic     |arpa            |1             |\n",
      "|367858 |beta       |arpa            |1             |\n",
      "|367859 |callic     |arpa            |1             |\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:10 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "23/09/04 12:21:10 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1970 bytes result sent to driver\n",
      "23/09/04 12:21:10 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 116 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:10 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:10 INFO DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 0.132 s\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "23/09/04 12:21:10 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.137047 s\n"
     ]
    }
   ],
   "source": [
    "# Save the `common_crawl` DataFrame to a series of parquet files\n",
    "common_crawl.write.parquet('./results/common_crawl/', mode=\"overwrite\")\n",
    "\n",
    "# Read from parquet directory\n",
    "common_crawl_domains = spark.read.parquet('./results/common_crawl/')\n",
    "\n",
    "# Display the first few rows of the DataFrame and the schema\n",
    "common_crawl_domains.show(5,truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96f34ede",
   "metadata": {},
   "source": [
    "## Querying Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d71895cf",
   "metadata": {},
   "source": [
    "Create a local temporary view from `common_crawl_domains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd04b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:10 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.1.123:52300 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:10 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.1.123:52300 in memory (size: 78.3 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view in the metadata for this `SparkSession`\n",
    "common_crawl_domains.createOrReplaceTempView(\"common_crawl_domains\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5c2ef4f",
   "metadata": {},
   "source": [
    "Calculate the total number of domains for each top-level domain in the dataset, using Dataframe and SQL methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f79679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:10 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:10 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/04 12:21:11 INFO CodeGenerator: Code generated in 68.407171 ms\n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 201.1 KiB, free 434.0 MiB)\n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.1.123:52300 (size: 35.2 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:11 INFO SparkContext: Created broadcast 13 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Registering RDD 31 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Got map stage job 7 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 40.5 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.1.123:52300 (size: 18.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:11 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:11 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:11 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8097 bytes) \n",
      "23/09/04 12:21:11 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "23/09/04 12:21:11 INFO CodeGenerator: Code generated in 11.827333 ms\n",
      "23/09/04 12:21:11 INFO CodeGenerator: Code generated in 6.055244 ms\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.1.123:52300 in memory (size: 6.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.1.123:52300 in memory (size: 35.2 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:11 INFO CodeGenerator: Code generated in 10.823516 ms\n",
      "23/09/04 12:21:11 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/results/common_crawl/part-00000-d507c219-514d-47a0-9cdd-d3c7b288e82d-c000.snappy.parquet, range: 0-639764, partition values: [empty row]\n",
      "23/09/04 12:21:11 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 3017 bytes result sent to driver\n",
      "23/09/04 12:21:11 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 230 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:11 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:11 INFO DAGScheduler: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.246 s\n",
      "23/09/04 12:21:11 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/09/04 12:21:11 INFO DAGScheduler: running: Set()\n",
      "23/09/04 12:21:11 INFO DAGScheduler: waiting: Set()\n",
      "23/09/04 12:21:11 INFO DAGScheduler: failed: Set()\n",
      "23/09/04 12:21:11 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/09/04 12:21:11 INFO CodeGenerator: Code generated in 12.543267 ms\n",
      "23/09/04 12:21:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/09/04 12:21:11 INFO CodeGenerator: Code generated in 20.155152 ms\n",
      "23/09/04 12:21:11 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Got job 8 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Final stage: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[35] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 41.3 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.1.123:52300 (size: 19.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:11 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[35] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:11 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:11 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (192.168.1.123, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/09/04 12:21:11 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)\n",
      "23/09/04 12:21:11 INFO ShuffleBlockFetcherIterator: Getting 1 (536.0 B) non-empty blocks including 1 (536.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/04 12:21:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
      "23/09/04 12:21:11 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 5669 bytes result sent to driver\n",
      "23/09/04 12:21:11 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 91 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:11 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:11 INFO DAGScheduler: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.104 s\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Job 8 finished: showString at NativeMethodAccessorImpl.java:0, took 0.119468 s\n",
      "23/09/04 12:21:11 INFO CodeGenerator: Code generated in 12.595587 ms\n",
      "23/09/04 12:21:11 INFO CodeGenerator: Code generated in 8.269984 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|top_level_domain|sum(num_subdomains)|\n",
      "+----------------+-------------------+\n",
      "|edu             |484438             |\n",
      "|gov             |85354              |\n",
      "|travel          |10768              |\n",
      "|coop            |8683               |\n",
      "|jobs            |6023               |\n",
      "|post            |143                |\n",
      "|map             |40                 |\n",
      "|arpa            |17                 |\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 201.1 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.1.123:52300 in memory (size: 18.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.1.123:52300 in memory (size: 19.3 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.1.123:52300 (size: 35.2 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:11 INFO SparkContext: Created broadcast 16 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.1.123:52300 in memory (size: 35.2 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Registering RDD 39 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Got map stage job 9 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[39] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 40.5 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:11 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:11 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.1.123:52300 (size: 18.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:11 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[39] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:11 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:11 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8097 bytes) \n",
      "23/09/04 12:21:11 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)\n",
      "23/09/04 12:21:11 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/results/common_crawl/part-00000-d507c219-514d-47a0-9cdd-d3c7b288e82d-c000.snappy.parquet, range: 0-639764, partition values: [empty row]\n",
      "23/09/04 12:21:12 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 3017 bytes result sent to driver\n",
      "23/09/04 12:21:12 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 75 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:12 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:12 INFO DAGScheduler: ShuffleMapStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 0.090 s\n",
      "23/09/04 12:21:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/09/04 12:21:12 INFO DAGScheduler: running: Set()\n",
      "23/09/04 12:21:12 INFO DAGScheduler: waiting: Set()\n",
      "23/09/04 12:21:12 INFO DAGScheduler: failed: Set()\n",
      "23/09/04 12:21:12 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/09/04 12:21:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|top_level_domain|sum(num_subdomains)|\n",
      "+----------------+-------------------+\n",
      "|edu             |484438             |\n",
      "|gov             |85354              |\n",
      "|travel          |10768              |\n",
      "|coop            |8683               |\n",
      "|jobs            |6023               |\n",
      "|post            |143                |\n",
      "|map             |40                 |\n",
      "|arpa            |17                 |\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:12 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Got job 10 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Final stage: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[43] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:12 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 41.1 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:12 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:12 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.1.123:52300 (size: 19.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:12 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[43] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:12 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:12 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (192.168.1.123, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/09/04 12:21:12 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)\n",
      "23/09/04 12:21:12 INFO ShuffleBlockFetcherIterator: Getting 1 (536.0 B) non-empty blocks including 1 (536.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/04 12:21:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "23/09/04 12:21:12 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 5669 bytes result sent to driver\n",
      "23/09/04 12:21:12 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 19 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:12 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:12 INFO DAGScheduler: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0) finished in 0.029 s\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Job 10 finished: showString at NativeMethodAccessorImpl.java:0, took 0.034336 s\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "common_crawl_domains\\\n",
    ".select([\"top_level_domain\", \"num_subdomains\"])\\\n",
    ".groupBy(\"top_level_domain\").sum(\"num_subdomains\")\\\n",
    ".orderBy(\"sum(num_subdomains)\", ascending = False)\\\n",
    ".show(truncate = False)\n",
    "\n",
    "# Aggregate the DataFrame using SQL\n",
    "query = \"\"\" SELECT top_level_domain, sum(num_subdomains)\n",
    "FROM common_crawl_domains\n",
    "GROUP BY top_level_domain \n",
    "ORDER BY sum(num_subdomains) DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abb11c33",
   "metadata": {},
   "source": [
    "Calculate the total number of subdomains for each top-level domain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "502578e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/04 12:21:12 INFO CodeGenerator: Code generated in 44.542911 ms\n",
      "23/09/04 12:21:12 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 201.3 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:12 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.1.123:52300 in memory (size: 19.3 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:12 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.1.123:52300 in memory (size: 18.3 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:12 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.1.123:52300 in memory (size: 35.2 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:12 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:12 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.1.123:52300 (size: 35.2 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:12 INFO SparkContext: Created broadcast 19 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Registering RDD 47 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Got map stage job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[47] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:12 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 42.3 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:12 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:12 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.1.123:52300 (size: 18.7 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:12 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[47] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:12 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:12 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8097 bytes) \n",
      "23/09/04 12:21:12 INFO Executor: Running task 0.0 in stage 13.0 (TID 11)\n",
      "23/09/04 12:21:12 INFO CodeGenerator: Code generated in 13.689454 ms\n",
      "23/09/04 12:21:12 INFO CodeGenerator: Code generated in 9.41199 ms\n",
      "23/09/04 12:21:12 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/results/common_crawl/part-00000-d507c219-514d-47a0-9cdd-d3c7b288e82d-c000.snappy.parquet, range: 0-639764, partition values: [empty row]\n",
      "23/09/04 12:21:12 INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 2974 bytes result sent to driver\n",
      "23/09/04 12:21:12 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 373 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:12 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:12 INFO DAGScheduler: ShuffleMapStage 13 (showString at NativeMethodAccessorImpl.java:0) finished in 0.385 s\n",
      "23/09/04 12:21:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/09/04 12:21:12 INFO DAGScheduler: running: Set()\n",
      "23/09/04 12:21:12 INFO DAGScheduler: waiting: Set()\n",
      "23/09/04 12:21:12 INFO DAGScheduler: failed: Set()\n",
      "23/09/04 12:21:12 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/09/04 12:21:12 INFO CodeGenerator: Code generated in 4.859293 ms\n",
      "23/09/04 12:21:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/09/04 12:21:12 INFO CodeGenerator: Code generated in 19.705813 ms\n",
      "23/09/04 12:21:12 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Got job 12 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Final stage: ResultStage 15 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[51] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:12 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 42.5 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:12 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 434.0 MiB)\n",
      "23/09/04 12:21:12 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.1.123:52300 (size: 19.6 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:12 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[51] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:12 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:12 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.1.123:52300 in memory (size: 18.7 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:12 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (192.168.1.123, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/09/04 12:21:12 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)\n",
      "23/09/04 12:21:12 INFO ShuffleBlockFetcherIterator: Getting 1 (1036.7 KiB) non-empty blocks including 1 (1036.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/04 12:21:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "23/09/04 12:21:12 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 6690 bytes result sent to driver\n",
      "23/09/04 12:21:12 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 163 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:12 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:12 INFO DAGScheduler: ResultStage 15 (showString at NativeMethodAccessorImpl.java:0) finished in 0.180 s\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "23/09/04 12:21:12 INFO DAGScheduler: Job 12 finished: showString at NativeMethodAccessorImpl.java:0, took 0.192572 s\n",
      "23/09/04 12:21:12 INFO CodeGenerator: Code generated in 6.816024 ms\n",
      "23/09/04 12:21:12 INFO CodeGenerator: Code generated in 8.67418 ms\n",
      "23/09/04 12:21:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/04 12:21:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 201.3 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.1.123:52300 in memory (size: 35.2 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.1.123:52300 in memory (size: 19.6 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.1.123:52300 (size: 35.2 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:13 INFO SparkContext: Created broadcast 22 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Registering RDD 55 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Got map stage job 13 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[55] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 42.4 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.1.123:52300 (size: 18.8 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:13 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[55] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:13 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:13 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8097 bytes) \n",
      "23/09/04 12:21:13 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)\n",
      "23/09/04 12:21:13 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/results/common_crawl/part-00000-d507c219-514d-47a0-9cdd-d3c7b288e82d-c000.snappy.parquet, range: 0-639764, partition values: [empty row]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+-------------------+\n",
      "|top_level_domain|domain  |sum(num_subdomains)|\n",
      "+----------------+--------+-------------------+\n",
      "|edu             |academia|9657               |\n",
      "|edu             |mit     |7114               |\n",
      "|edu             |stanford|7015               |\n",
      "|edu             |harvard |5497               |\n",
      "|edu             |wisc    |5376               |\n",
      "|gov             |nasa    |5370               |\n",
      "|edu             |tamu    |5088               |\n",
      "|edu             |berkeley|4874               |\n",
      "|edu             |umn     |4748               |\n",
      "|edu             |ncsu    |4448               |\n",
      "|edu             |cuny    |4260               |\n",
      "|edu             |unc     |4249               |\n",
      "|edu             |cornell |4235               |\n",
      "|edu             |ucla    |4217               |\n",
      "|edu             |ucsd    |4122               |\n",
      "|edu             |ucdavis |4105               |\n",
      "|edu             |umich   |4067               |\n",
      "|edu             |psu     |3840               |\n",
      "|edu             |msu     |3830               |\n",
      "|edu             |ufl     |3795               |\n",
      "+----------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:13 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 3017 bytes result sent to driver\n",
      "23/09/04 12:21:13 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 294 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:13 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:13 INFO DAGScheduler: ShuffleMapStage 16 (showString at NativeMethodAccessorImpl.java:0) finished in 0.302 s\n",
      "23/09/04 12:21:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/09/04 12:21:13 INFO DAGScheduler: running: Set()\n",
      "23/09/04 12:21:13 INFO DAGScheduler: waiting: Set()\n",
      "23/09/04 12:21:13 INFO DAGScheduler: failed: Set()\n",
      "23/09/04 12:21:13 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/09/04 12:21:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/09/04 12:21:13 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Got job 14 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Final stage: ResultStage 18 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[59] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 42.3 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 434.0 MiB)\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 192.168.1.123:52300 (size: 19.6 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:13 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[59] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:13 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:13 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (192.168.1.123, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/09/04 12:21:13 INFO Executor: Running task 0.0 in stage 18.0 (TID 14)\n",
      "23/09/04 12:21:13 INFO ShuffleBlockFetcherIterator: Getting 1 (1036.7 KiB) non-empty blocks including 1 (1036.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/04 12:21:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 192.168.1.123:52300 in memory (size: 18.8 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:13 INFO Executor: Finished task 0.0 in stage 18.0 (TID 14). 6733 bytes result sent to driver\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+-------------------+\n",
      "|top_level_domain|domain  |sum(num_subdomains)|\n",
      "+----------------+--------+-------------------+\n",
      "|edu             |academia|9657               |\n",
      "|edu             |mit     |7114               |\n",
      "|edu             |stanford|7015               |\n",
      "|edu             |harvard |5497               |\n",
      "|edu             |wisc    |5376               |\n",
      "|gov             |nasa    |5370               |\n",
      "|edu             |tamu    |5088               |\n",
      "|edu             |berkeley|4874               |\n",
      "|edu             |umn     |4748               |\n",
      "|edu             |ncsu    |4448               |\n",
      "|edu             |cuny    |4260               |\n",
      "|edu             |unc     |4249               |\n",
      "|edu             |cornell |4235               |\n",
      "|edu             |ucla    |4217               |\n",
      "|edu             |ucsd    |4122               |\n",
      "|edu             |ucdavis |4105               |\n",
      "|edu             |umich   |4067               |\n",
      "|edu             |psu     |3840               |\n",
      "|edu             |msu     |3830               |\n",
      "|edu             |ufl     |3795               |\n",
      "+----------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:13 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 132 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:13 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:13 INFO DAGScheduler: ResultStage 18 (showString at NativeMethodAccessorImpl.java:0) finished in 0.144 s\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Job 14 finished: showString at NativeMethodAccessorImpl.java:0, took 0.155534 s\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "common_crawl_domains\\\n",
    ".select([\"top_level_domain\",\"domain\", \"num_subdomains\"])\\\n",
    ".groupBy([\"top_level_domain\", \"domain\"]).sum(\"num_subdomains\")\\\n",
    ".orderBy(\"sum(num_subdomains)\", ascending = False)\\\n",
    ".show(truncate = False)\n",
    "\n",
    "# Aggregate the DataFrame using SQL\n",
    "query = \"\"\" SELECT top_level_domain, domain, sum(num_subdomains)\n",
    "FROM common_crawl_domains\n",
    "GROUP BY top_level_domain, domain \n",
    "ORDER BY sum(num_subdomains) DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate = False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a2dfea3",
   "metadata": {},
   "source": [
    "How many sub-domains does `nps.gov` have? Filter the dataset to that website's entry, display the columns `top_level_domain`, `domain`, and `num_subdomains` in your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b45051e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(top_level_domain),IsNotNull(domain),EqualTo(top_level_domain,gov),EqualTo(domain,nps)\n",
      "23/09/04 12:21:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(top_level_domain#98),isnotnull(domain#97),(top_level_domain#98 = gov),(domain#97 = nps)\n",
      "23/09/04 12:21:13 INFO CodeGenerator: Code generated in 14.352805 ms\n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 201.4 KiB, free 433.9 MiB)\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 192.168.1.123:52300 in memory (size: 35.2 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 192.168.1.123:52300 in memory (size: 19.6 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 192.168.1.123:52300 (size: 35.2 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:13 INFO SparkContext: Created broadcast 25 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:13 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Got job 15 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Final stage: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[63] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.9 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:13 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:13 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 192.168.1.123:52300 (size: 6.7 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:13 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[63] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:13 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:13 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes) \n",
      "23/09/04 12:21:13 INFO Executor: Running task 0.0 in stage 19.0 (TID 15)\n",
      "23/09/04 12:21:13 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/results/common_crawl/part-00000-d507c219-514d-47a0-9cdd-d3c7b288e82d-c000.snappy.parquet, range: 0-639764, partition values: [empty row]\n",
      "23/09/04 12:21:13 INFO FilterCompat: Filtering using predicate: and(and(and(noteq(top_level_domain, null), noteq(domain, null)), eq(top_level_domain, Binary{\"gov\"})), eq(domain, Binary{\"nps\"}))\n",
      "23/09/04 12:21:13 INFO Executor: Finished task 0.0 in stage 19.0 (TID 15). 1864 bytes result sent to driver\n",
      "23/09/04 12:21:13 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 89 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:13 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:13 INFO DAGScheduler: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0) finished in 0.111 s\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "23/09/04 12:21:13 INFO DAGScheduler: Job 15 finished: showString at NativeMethodAccessorImpl.java:0, took 0.113616 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------------+--------------+\n",
      "|site_id |domain|top_level_domain|num_subdomains|\n",
      "+--------+------+----------------+--------------+\n",
      "|57661852|nps   |gov             |178           |\n",
      "+--------+------+----------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:14 INFO FileSourceStrategy: Pushed Filters: IsNotNull(top_level_domain),IsNotNull(domain),EqualTo(top_level_domain,gov),EqualTo(domain,nps)\n",
      "23/09/04 12:21:14 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(top_level_domain#98),isnotnull(domain#97),(top_level_domain#98 = gov),(domain#97 = nps)\n",
      "23/09/04 12:21:14 INFO CodeGenerator: Code generated in 31.846132 ms\n",
      "23/09/04 12:21:14 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)\n",
      "23/09/04 12:21:14 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 192.168.1.123:52300 in memory (size: 6.7 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:14 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 192.168.1.123:52300 in memory (size: 35.2 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:14 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.2 MiB)\n",
      "23/09/04 12:21:14 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 192.168.1.123:52300 (size: 35.2 KiB, free: 434.4 MiB)\n",
      "23/09/04 12:21:14 INFO SparkContext: Created broadcast 27 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Registering RDD 67 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Got map stage job 16 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[67] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:14 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 44.3 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:14 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:14 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 192.168.1.123:52300 (size: 19.5 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:14 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[67] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:14 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:14 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (192.168.1.123, executor driver, partition 0, PROCESS_LOCAL, 8097 bytes) \n",
      "23/09/04 12:21:14 INFO Executor: Running task 0.0 in stage 20.0 (TID 16)\n",
      "23/09/04 12:21:14 INFO FileScanRDD: Reading File path: file:///Users/victorurdaneta/Desktop/Documentos%20Personales/Personal%20Growth/CodeAcademy/Analyze%20Common%20Crawl%20Data%20with%20PySpark/results/common_crawl/part-00000-d507c219-514d-47a0-9cdd-d3c7b288e82d-c000.snappy.parquet, range: 0-639764, partition values: [empty row]\n",
      "23/09/04 12:21:14 INFO FilterCompat: Filtering using predicate: and(and(and(noteq(top_level_domain, null), noteq(domain, null)), eq(top_level_domain, Binary{\"gov\"})), eq(domain, Binary{\"nps\"}))\n",
      "23/09/04 12:21:14 INFO Executor: Finished task 0.0 in stage 20.0 (TID 16). 3073 bytes result sent to driver\n",
      "23/09/04 12:21:14 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 66 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:14 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:14 INFO DAGScheduler: ShuffleMapStage 20 (showString at NativeMethodAccessorImpl.java:0) finished in 0.078 s\n",
      "23/09/04 12:21:14 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/09/04 12:21:14 INFO DAGScheduler: running: Set()\n",
      "23/09/04 12:21:14 INFO DAGScheduler: waiting: Set()\n",
      "23/09/04 12:21:14 INFO DAGScheduler: failed: Set()\n",
      "23/09/04 12:21:14 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------------------+\n",
      "|top_level_domain|domain|sum(num_subdomains)|\n",
      "+----------------+------+-------------------+\n",
      "|gov             |nps   |178                |\n",
      "+----------------+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:14 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/09/04 12:21:14 INFO CodeGenerator: Code generated in 26.253337 ms\n",
      "23/09/04 12:21:14 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Got job 17 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Final stage: ResultStage 22 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[70] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/04 12:21:14 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 44.2 KiB, free 434.1 MiB)\n",
      "23/09/04 12:21:14 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.0 MiB)\n",
      "23/09/04 12:21:14 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 192.168.1.123:52300 (size: 20.4 KiB, free: 434.3 MiB)\n",
      "23/09/04 12:21:14 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[70] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/04 12:21:14 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "23/09/04 12:21:14 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (192.168.1.123, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/09/04 12:21:14 INFO Executor: Running task 0.0 in stage 22.0 (TID 17)\n",
      "23/09/04 12:21:14 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/04 12:21:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/09/04 12:21:14 INFO Executor: Finished task 0.0 in stage 22.0 (TID 17). 5407 bytes result sent to driver\n",
      "23/09/04 12:21:14 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 15 ms on 192.168.1.123 (executor driver) (1/1)\n",
      "23/09/04 12:21:14 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "23/09/04 12:21:14 INFO DAGScheduler: ResultStage 22 (showString at NativeMethodAccessorImpl.java:0) finished in 0.022 s\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/04 12:21:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "23/09/04 12:21:14 INFO DAGScheduler: Job 17 finished: showString at NativeMethodAccessorImpl.java:0, took 0.025303 s\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame using DataFrame Methods\n",
    "common_crawl_domains\\\n",
    ".filter(common_crawl_domains.top_level_domain == \"gov\")\\\n",
    ".filter(common_crawl_domains.domain == \"nps\")\\\n",
    ".show(truncate = False)\n",
    "\n",
    "# Filter the DataFrame using SQL\n",
    "query = \"\"\" SELECT top_level_domain, domain, sum(num_subdomains)\n",
    "FROM common_crawl_domains\n",
    "WHERE top_level_domain = 'gov'and domain = 'nps'\n",
    "GROUP BY top_level_domain, domain \n",
    "ORDER BY sum(num_subdomains) DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate = False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7336d454",
   "metadata": {},
   "source": [
    "Close the `SparkSession` and underlying `sparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2233037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:21:14 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "23/09/04 12:21:14 INFO SparkUI: Stopped Spark web UI at http://192.168.1.123:4040\n",
      "23/09/04 12:21:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/09/04 12:21:14 INFO MemoryStore: MemoryStore cleared\n",
      "23/09/04 12:21:14 INFO BlockManager: BlockManager stopped\n",
      "23/09/04 12:21:14 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/09/04 12:21:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/09/04 12:21:14 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# Stop the notebook's `SparkSession` and `sparkContext`\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
